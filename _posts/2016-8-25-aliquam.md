---
layout: post
title: Everything Everywhere
description: NS, RL
image: assets/images/pic01.jpg
---

We have to address the problem that neurons diversify the signal passage between them with neurotransmitters. The problem begins with the fact the each cell has multiple type of ions, especially presence of multiple types of ligand gated channels that take part in signal in each synapse, each convey information differently

The problem is, even looking at it from a very high level, you have to at least divide receptors in synapses into two major categories, excitatory and inhibitory receptors. So, for example, an incoming stimuli can cause a neuron to start firing with the excitatory neurotransmitter, and then it keeps firing, and there is information signaling and processing that's going on in layers after that neuron because of this firing, whilst there could also be new incoming information from the same synapse that caused the first activation, and then this time it could be an inhibitory neurotransmitter that stops it from firing, so there is a change in the pattern. So, in that way, it could be argued that we can control the neural nature of the world to control the same synapse in different ways.

Since in artificial neural networks, every pass is staggered because of inputs. So, what I mean by that is, every input is the one that triggers a forward pass. But then, in real neural networks, there could be activations and signal passes going around inside the network, while it also can take in and processes incoming signals while the first pass itself hasn't finished. Which is like the network is thinking while also awaiting and receiving new stimulus.

Each piece of information encoded by a signal from a neuron not only has depth but also has breadth to it. So, a neuron, the signals between neurons have a depth, a breadth, and are also a function of time at the same time. And the state of that information is a function of time and it changes with time. So, the breadth and depth both change with time.

One way of addressing this problem would be to create neuronal states, which can be manipulated using each pass, so we can still keep the programming structure of staggered inputs that activate a network, whilst also incorporating the idea that a neuron can also fire, depending on what its previous state was, which is kind of the recurrent nature, I guess, but also each neuron can fire as sort of a continuous time delineation.

Each piece of information encoded by a signal from a neuron not only has depth but also has breadth to it. So, a neuron, the signals between neurons have a depth, a breadth, and are also a function of time at the same time. And the state of that information is a function of time and it changes with time. So, the breadth and depth both change with time.

Why do we don’t need a synchronized clock?
We still haven’t addressed time discretisation in the Classical Von-Neumann architecture computer, where every piece of computer is still in constant predefined discrete time clock cycles. Well, there are electronic solutions to these problems like Neuromorphic computing which run without a synchronizing clock, but still we should be able to get over this problem with concurrent programming, at least to a certain level of time continuity (Billionth of a second),  even in Von-Neumann architecture. 

There is already extensive research being done on this.  above mentioned neuromorphic computing. There are multiple ways you could achieve this sort of no-synchronized clock chips that exclusively work only for neural networks. There are ways you could use memristors, or you could use conventional RAMs and warn you in architecture, but then they are adapted in a way that you don't need synchronized clocks, but still use the warn you in architecture, which is sort of in between this completely neuromorphic technology where you can use memristors to actually store weights in transistors or in chips, in the silicon. So that is, although in my field I completely don't enter this sort of realm of electronics, I just want to mention that my models could very easily be used on those, very easily and proactively be used on these sort of neuromorphic computing, which I know the industry is very desperate for the software development community to develop code for neuromorphic chips.

Language Acquisition:
The problem of language might be the most valuable and also hardest part. In spite of every close evolutionary link to 100s of  species, homo sapiens are the only species to have mastered language. Problem looks starker when we find that except for the slightly larger Broca's area and the ~1-2% difference in genetic code, there isn't much difference between primates and humans. Although the completion of the Human Genome project and finding of genes like FOXP2 have led to the discovery of direct links to Language Ability in humans, they do not come close to fully explaining language ability in Humans. But even our close evolutionary partner, but not biologically close organisms like drosophila have FOXP2 gene, Also the fact that complex organisms like Humans the interplay of genes is what leads to fully developed features, and not single genes themselves makes it hard to find discernable biological architecture for Language Ability. 

We not only inherently design proteins and enzymes through evolution we also inherit behavior and goals. The biological reinforcements within the cerebral cortex. Once we have a working model of an agent that operates as an interplay between a reward function, the Using game theory to rationalize and forcefully control interaction between agents and letting them evolve with constraints can help us identify language goals.

