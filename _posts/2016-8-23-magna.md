---
layout: post
title: Magna
description: 
image: assets/images/pic03.jpg
---

![test image]({{ site.url | absolute_path}}/assets/images/pic03.jpg)

In Richard Sutton's book on reinforcement learning, there is a chapter on the first, the slot thing, has learned without using any observation from the environment purely on the reward it gets. And then he devices an algorithm, like he talks about the algorithm which can do that, which is like the slot machine we can learn from without the observation of the state of the environment, states. But then he also gives explanation as to why it's important to observe states and how the problem can be, how complex problem it's useful to observe states of the environment and then make decisions based on that. So that's when probability of doing something with respect to state and action comes in. And that's when the state itself comes into the reinforcement learning game. And in a similar way, we have to redefine how we define state.


So, in a way, essentially what I'm trying to say is it kind of reminds us of the actor-critic methods in reinforcement learning, where there is a separate actor and a separate critic. A critic is essentially a network which learns the value functions and outputs the value of certain of the state-action pairs. And then there is the actor that learns how to take actions based on the values it has learned. And essentially, that's what I'm trying to do. But then the way I am trying to apply the problem is more derived from human, more derived from biological brains, that we can sort of, I'm also trying to add another part, which is the short-term memory component, where every piece of information that comes in is stored and evaluated based on what significance it has to a future reward, which is what our brain does. And our brain weighs every memory emotionally, and then sees if that is important for a future reward, and essentially storing which is important. I want to do something in a similar way. And also, the approach to the actor-critic itself is different, because what I am trying to do is trying to generalize the reward. A reward of removing that from the action network, so the actor network, and then trying to get a reward network to learn the rewards instead of an actor network.
So what I'm essentially suggesting is that trying to take a different approach to the same problem but then taking more biologically driven approaches so that we can sort of trace back, backtrace and see how these networks interact with each other and use that to kind of address biological problems too. Whereas with Reinforced Learning we're kind of using more sort of mathematically and statistical reasoning methods to kind of build these models which in itself is useful in a very narrow field.
