---
layout: post
title: Ipsum
description: Whats New
image: assets/images/pic04.jpg
---

I think more than enough researchers and research papers, including the most recent one by Geoffrey Hinton, named the Forward-Forward Algorithm, which is making buzz in the machine learning community, have sort of agreed that backpropagation and forward propagation are too old for a new era of machine learning and something has to be done about it, which is also the case with my model where if you think about using backpropagation and waiting till the end of the batch to collect all the errors, calculate the sum and the loss, and then keep all the information in the hidden layers intact, and then do the backpropagation, it's going to be very difficult when you have multiple networks interacting with each other and trying to activate them in the middle and everything. So you have to invent a new method and I'm going to coin the term double MAC learning or DMAC, where I'm using just two MAC operations for both the forward pass and the learning (weight update) during the forward pass itself.
